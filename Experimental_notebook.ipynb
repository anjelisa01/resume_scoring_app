{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOgUUTjABih8W5VJ0RE3Xz6"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#All install and imports"
      ],
      "metadata": {
        "id": "0oBIofrpQ3LI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#install\n",
        "!pip install pymupdf\n",
        "!pip install sentence-transformers\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArJ0vw70StVc",
        "outputId": "6569c115-494f-4b5d-d995-17c76976ae78"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.11/dist-packages (1.25.5)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.51.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.31.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.4.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "jXwYxTr7QdvD"
      },
      "outputs": [],
      "source": [
        "#imports\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import fitz  # PyMuPDF\n",
        "import re\n",
        "import unicodedata\n",
        "import re\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PDF"
      ],
      "metadata": {
        "id": "2GTFtIesQ_OK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Functions"
      ],
      "metadata": {
        "id": "APhHlin2RDvp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Extract pdf resume"
      ],
      "metadata": {
        "id": "eEaAXztbS251"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_pdf(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    full_text = \"\"\n",
        "    for page in doc:\n",
        "        full_text += page.get_text() + \"\\n\"\n",
        "    return full_text\n"
      ],
      "metadata": {
        "id": "NWowhWxWU2nP"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Initial cleaning"
      ],
      "metadata": {
        "id": "BV4AkRPNS7aQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_resume_text(text):\n",
        "    # Normalize Unicode (e.g. accents, special characters)\n",
        "    text = unicodedata.normalize(\"NFKD\", text)\n",
        "\n",
        "    # Remove zero-width spaces and non-printable characters\n",
        "    text = re.sub(r'[\\u200B-\\u200D\\uFEFF]', '', text)\n",
        "\n",
        "    # Remove multiple spaces and fix inconsistent newlines\n",
        "    text = re.sub(r'[ \\t]+', ' ', text)\n",
        "    text = re.sub(r'\\n{2,}', '\\n', text)\n",
        "\n",
        "    # Strip each line\n",
        "    lines = [line.strip() for line in text.splitlines() if line.strip()]\n",
        "    return '\\n'.join(lines)"
      ],
      "metadata": {
        "id": "DzQwVa5FS1zA"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Sectioning"
      ],
      "metadata": {
        "id": "b1qxUmxPS_yZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_resume_into_sections(text):\n",
        "    sections = {\n",
        "        \"contact\": \"\",\n",
        "        \"summary\": \"\",\n",
        "        \"skills\": \"\",\n",
        "        \"experience\": \"\",\n",
        "        \"education\": \"\",\n",
        "        \"certifications\": \"\",\n",
        "        \"projects\": \"\",\n",
        "        \"others\": \"\"\n",
        "    }\n",
        "\n",
        "    patterns = {\n",
        "        \"summary\": r\"(summary|professional summary|profile)\",\n",
        "        \"skills\": r\"(skills|technical skills)\",\n",
        "        \"experience\": r\"(experience|work experience|employment history)\",\n",
        "        \"education\": r\"(education|academic background)\",\n",
        "        \"certifications\": r\"(certifications|licenses)\",\n",
        "        \"projects\": r\"(projects|project highlights)\",\n",
        "        \"others\": r\"(interests|languages|volunteer|publications|additional information)\"\n",
        "    }\n",
        "\n",
        "    lines = text.splitlines()\n",
        "    current_section = \"contact\"  # Assume resume starts with contact\n",
        "\n",
        "    for line in lines:\n",
        "        clean_line = line.strip().lower()\n",
        "\n",
        "        # Check if this line is a section header\n",
        "        for section, pattern in patterns.items():\n",
        "            if re.search(rf\"^{pattern}[:\\s]*$\", clean_line):\n",
        "                current_section = section\n",
        "                break\n",
        "        else:\n",
        "            sections[current_section] += line + '\\n'\n",
        "\n",
        "    return sections\n"
      ],
      "metadata": {
        "id": "G0tJOfiNTDOS"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Second cleaning-sectioned"
      ],
      "metadata": {
        "id": "PMaCWDKmTFvM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def clean_resume_section(text):\n",
        "#     # Normalize unicode\n",
        "#     text = unicodedata.normalize(\"NFKD\", text)\n",
        "#     # Remove invisible characters\n",
        "#     text = re.sub(r'[\\u200B-\\u200D\\uFEFF]', '', text)\n",
        "#     # Remove common bullet characters and excess punctuation\n",
        "#     text = re.sub(r'[•●◦▪️]', '', text)\n",
        "#     text = re.sub(r'[-–—]', ' ', text)\n",
        "#     # Normalize whitespace\n",
        "#     text = re.sub(r'\\s+', ' ', text).strip()\n",
        "#     # Ensure proper sentence endings\n",
        "#     # text = re.sub(r'(?<!\\w)([A-Z][a-z]+)', r'\\n\\1', text)  # crude line breaks before new capitalized words\n",
        "#     return text.strip()\n",
        "# def prepare_sections_for_semantic_scoring(section_dict):\n",
        "#     cleaned_sections = {}\n",
        "#     for section, content in section_dict.items():\n",
        "#         cleaned_text = clean_resume_section(content)\n",
        "#         cleaned_sections[section] = cleaned_text\n",
        "#     return cleaned_sections"
      ],
      "metadata": {
        "id": "Y6AdSRj6TKpQ"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import unicodedata\n",
        "\n",
        "def clean_resume_section(text):\n",
        "    # Unicode normalization\n",
        "    text = unicodedata.normalize(\"NFKD\", text)\n",
        "\n",
        "    # Remove zero-width spaces and similar non-printables\n",
        "    text = re.sub(r'[\\u200B-\\u200D\\uFEFF]', '', text)\n",
        "\n",
        "    # Standardize bullet points (replace all types with `-`)\n",
        "    text = re.sub(r'[•●◦▪️]', '-', text)\n",
        "\n",
        "    # Normalize dashes (– — − → -)\n",
        "    text = re.sub(r'[–—−]', '-', text)\n",
        "\n",
        "    # Fix spaces around separators\n",
        "    text = re.sub(r'\\s*[-]\\s*', ' - ', text)\n",
        "\n",
        "    # Remove leading/trailing hyphens if orphaned\n",
        "    text = re.sub(r'^[-\\s]+|[-\\s]+$', '', text)\n",
        "\n",
        "    # Remove duplicate dashes (e.g., \"--\")\n",
        "    text = re.sub(r'-{2,}', '-', text)\n",
        "\n",
        "    # Ensure single spaces\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # Preserve line breaks where they indicate structure\n",
        "    text = text.replace('. ', '.\\n')  # Break long lines into per-sentence structure\n",
        "\n",
        "    # Final strip\n",
        "    return text.strip()\n",
        "def prepare_sections_for_semantic_scoring(section_dict):\n",
        "    cleaned_sections = {}\n",
        "    for section, content in section_dict.items():\n",
        "        cleaned_text = clean_resume_section(content)\n",
        "        cleaned_sections[section] = cleaned_text\n",
        "    return cleaned_sections\n"
      ],
      "metadata": {
        "id": "1eR96wlA9tIe"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Convert to natural languange"
      ],
      "metadata": {
        "id": "PgJ-PSDqTL11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def others_list_to_sentence(skill_text):\n",
        "#     # Remove bullets and split by common delimiters\n",
        "#     skills = re.split(r'[,\\n;]', skill_text)\n",
        "#     skills = [s.strip() for s in skills if s.strip()]\n",
        "#     if not skills:\n",
        "#         return \"\"\n",
        "#     return \"Others things i have participate in thats relevant fot this role are \" + \", \".join(skills[:-1]) + f\", and {skills[-1]}.\"\n",
        "# def certifications_list_to_sentence(skill_text):\n",
        "#     # Remove bullets and split by common delimiters\n",
        "#     skills = re.split(r'[,\\n;]', skill_text)\n",
        "#     skills = [s.strip() for s in skills if s.strip()]\n",
        "#     if not skills:\n",
        "#         return \"\"\n",
        "#     return \"I hold certifications including \" + \", \".join(skills[:-1]) + f\", and {skills[-1]}.\"\n",
        "# def skill_list_to_sentence(skill_text):\n",
        "#     # Remove bullets and split by common delimiters\n",
        "#     skills = re.split(r'[,\\n;]', skill_text)\n",
        "#     skills = [s.strip() for s in skills if s.strip()]\n",
        "#     if not skills:\n",
        "#         return \"\"\n",
        "\n",
        "#     return \"This candidate is proficient in \" + \", \".join(skills[:-1]) + f\", and {skills[-1]}.\"\n",
        "# def bullet_points_to_sentences(text):\n",
        "#     bullets = re.split(r'\\n|•|- ', text)\n",
        "#     sentences = []\n",
        "#     for bullet in bullets:\n",
        "#         b = bullet.strip()\n",
        "#         if b and not b.endswith('.'):\n",
        "#             b += '.'\n",
        "#         if b:\n",
        "#             sentences.append(b)\n",
        "#     return \" \".join(sentences)\n",
        "# #========================================================\n",
        "# def naturalize_resume_sections(sections):\n",
        "#     natural_sections = {}\n",
        "#     for section, content in sections.items():\n",
        "#         if section == 'skills':\n",
        "#             natural_sections[section] = skill_list_to_sentence(content)\n",
        "#         elif section == 'others':\n",
        "#             natural_sections[section] = others_list_to_sentence(content)\n",
        "#         elif section == 'certifications':\n",
        "#             natural_sections[section] = certifications_list_to_sentence(content)\n",
        "#         elif section in ['experience', 'projects']:\n",
        "#             natural_sections[section] = bullet_points_to_sentences(content)\n",
        "#         else:\n",
        "#             natural_sections[section] = content  # already in sentence form\n",
        "#     return natural_sections"
      ],
      "metadata": {
        "id": "RHa3KKUtTQKx"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def bullet_points_to_sentences(text):\n",
        "    # Split by common bullet formats (dash, bullet, newline)\n",
        "    bullets = re.split(r'(?:\\n|^)[\\-\\•●◦▪️] ?', text)\n",
        "    sentences = []\n",
        "    for bullet in bullets:\n",
        "        b = bullet.strip()\n",
        "        if not b:\n",
        "            continue\n",
        "        # Ensure the sentence ends with a period\n",
        "        if not b.endswith('.'):\n",
        "            b += '.'\n",
        "        sentences.append(b)\n",
        "    return ' '.join(sentences)\n",
        "\n",
        "def skill_list_to_sentence(skill_text):\n",
        "    skills = re.split(r'[,\\n;]', skill_text)\n",
        "    skills = [s.strip() for s in skills if s.strip()]\n",
        "    if not skills:\n",
        "        return \"\"\n",
        "    if len(skills) == 1:\n",
        "        return f\"This candidate is proficient in {skills[0]}.\"\n",
        "    return \"This candidate is proficient in \" + \", \".join(skills[:-1]) + f\", and {skills[-1]}.\"\n",
        "\n",
        "def certifications_list_to_sentence(cert_text):\n",
        "    certs = re.split(r'[,\\n;]', cert_text)\n",
        "    certs = [c.strip().rstrip('.') for c in certs if c.strip()]\n",
        "    if not certs:\n",
        "        return \"\"\n",
        "    if len(certs) == 1:\n",
        "        return f\"I hold the certification: {certs[0]}.\"\n",
        "    return \"I hold certifications including \" + \", \".join(certs[:-1]) + f\", and {certs[-1]}.\"\n",
        "\n",
        "def others_list_to_sentence(others_text):\n",
        "    items = re.split(r'[,\\n;]', others_text)\n",
        "    items = [i.strip().rstrip('.') for i in items if i.strip()]\n",
        "    if not items:\n",
        "        return \"\"\n",
        "    if len(items) == 1:\n",
        "        return f\"Other relevant participation includes {items[0]}.\"\n",
        "    return \"Other relevant participation includes \" + \", \".join(items[:-1]) + f\", and {items[-1]}.\"\n",
        "\n",
        "# ==========================================\n",
        "\n",
        "def naturalize_resume_sections(sections):\n",
        "    natural_sections = {}\n",
        "    for section, content in sections.items():\n",
        "        if section == 'skills':\n",
        "            natural_sections[section] = skill_list_to_sentence(content)\n",
        "        elif section == 'others':\n",
        "            natural_sections[section] = others_list_to_sentence(content)\n",
        "        elif section == 'certifications':\n",
        "            natural_sections[section] = certifications_list_to_sentence(content)\n",
        "        elif section in ['experience', 'projects']:\n",
        "            natural_sections[section] = bullet_points_to_sentences(content)\n",
        "        else:\n",
        "            # Clean up rogue spacing/punctuation in regular paragraphs\n",
        "            content = content.strip()\n",
        "            if content and not content.endswith('.'):\n",
        "                content += '.'\n",
        "            natural_sections[section] = content\n",
        "    return natural_sections\n"
      ],
      "metadata": {
        "id": "bXve4lmq_MZA"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Final cleaning"
      ],
      "metadata": {
        "id": "QFmRk7xyTRom"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "jadi manfaatin delimiter untuk bedain header, bullet points dll. setelah sudah dpt bentuk natural languange baru dilakukan final cleaning"
      ],
      "metadata": {
        "id": "h33Rgsy2TUtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def clean_text(text):\n",
        "#     # text = unidecode.unidecode(text)  # Remove accents\n",
        "#     text = re.sub(r'\\s+', ' ', text)  # Normalize whitespace\n",
        "#     text = re.sub(r'[^a-zA-Z0-9.,;!?()\\[\\] ]+', '', text)  # Remove special characters\n",
        "#     # text = text.lower()  # Lowercase\n",
        "#     return text\n",
        "# #cleaning dict values\n",
        "# def clean_dict_values(data_dict):\n",
        "#     return {key: clean_text(value) for key, value in data_dict.items()}\n",
        "# # Assume `sections` is the output from your previous dictionary splitting function"
      ],
      "metadata": {
        "id": "xSZci929TWzw"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    text = re.sub(r'\\s*[-–—]\\s*', '-', text)  # Restore hyphens like front-end\n",
        "    text = re.sub(r'\\|\\s*', ', ', text)       # Convert pipes to commas\n",
        "    text = re.sub(r'\\s{2,}', ' ', text)       # Collapse excessive spacing\n",
        "    text = re.sub(r'\\s*\\.\\s*', '. ', text)    # Ensure space after periods\n",
        "    text = re.sub(r'\\s*,\\s*', ', ', text)     # Normalize commas\n",
        "    text = re.sub(r'\\.\\s*\\.', '.', text)      # Remove double periods\n",
        "    text = text.strip()\n",
        "    return text\n",
        "# #cleaning dict values\n",
        "def clean_dict_values(data_dict):\n",
        "    return {key: clean_text(value) for key, value in data_dict.items()}\n"
      ],
      "metadata": {
        "id": "tzFb1FYjAh6j"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Master functions"
      ],
      "metadata": {
        "id": "A2clZ6zQRGgO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pdf_processing(file_path): #nanti di app.py panggil dgn pdf_file.name\n",
        "  resume_raw=extract_text_from_pdf(file_path)\n",
        "  #initial cleaning\n",
        "  cleaned=clean_resume_text(resume_raw)\n",
        "  #sectioning\n",
        "  sections = split_resume_into_sections(cleaned)\n",
        "  #second cleaning for the sectioned text\n",
        "  cleaned_sections = prepare_sections_for_semantic_scoring(sections)\n",
        "  #natural langunage format\n",
        "  naturalized=naturalize_resume_sections(cleaned_sections)\n",
        "  # final cleaning ( final result)\n",
        "  final = clean_dict_values(naturalized)\n",
        "  return final"
      ],
      "metadata": {
        "id": "_m3MBm7lT4j5"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Test"
      ],
      "metadata": {
        "id": "yBmw4x9ZRIzc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path='MOCK-Alex_Johnson_Resume.pdf'\n",
        "pdf_resume_preprocessed=pdf_processing(pdf_path)"
      ],
      "metadata": {
        "id": "ldufDs-V71lh"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_resume_preprocessed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7zm7zXi94dG",
        "outputId": "1671a200-af91-43c1-d6c9-b93139b677ab"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'contact': 'Alex Johnson (123) 456-7890, alexjohnson. dev@email. com, github. com/alexjohnson-dev, alexjohnson. dev.',\n",
              " 'summary': 'Junior Web Developer with a strong foundation in front-end and back-end technologies. Skilled in building responsive, user-centric websites and applications. Passionate about learning new technologies and delivering high-quality code.',\n",
              " 'skills': 'This candidate is proficient in Languages: HTML5, CSS3, JavaScript (ES6+), Python Frameworks/Libraries: React. js, Node. js, Express. js, Bootstrap Databases: MongoDB, MySQL Tools: Git, GitHub, Visual Studio Code, Figma, Postman Other: RESTful APIs, Responsive Design, and Agile Methodology.',\n",
              " 'experience': 'Web Development Intern, BrightWave Solutions, Remote, Jan 2024-Apr 2024-Developed and maintained front-end components using React. js and Bootstrap. Built RESTful APIs with Node. js and Express. js. Created mobile-first responsive designs with UX teams. Improved website performance by 20%. Freelance Web Developer, Self-Employed, Remote, May 2023-Dec 2023-Designed and developed custom websites for local businesses. Integrated third-party APIs (Google Maps, Stripe). Provided maintenance and support for client websites.',\n",
              " 'education': 'Bachelor of Science in Computer Science, University of Springfield, Springfield, IL, May 2023 Relevant Coursework: Web Development, Database Systems, Software Engineering, Human-Computer Interaction.',\n",
              " 'certifications': 'I hold certifications including Responsive Web Design Certification (freeCodeCamp, 2022)-JavaScript Algorithms and Data Structures Certification (freeCodeCamp, and 2023).',\n",
              " 'projects': 'TaskMaster Web App-Built a full-stack task management app using React. js, Node. js, and MongoDB. Implemented user authentication and real-time notifications. Personal Portfolio Website-Designed and developed a personal website to showcase projects and blog posts. Focused on accessibility and responsive design.',\n",
              " 'others': 'Other relevant participation includes Open-source contributor, and-Volunteer coding mentor at CodeClub Springfield.'}"
            ]
          },
          "metadata": {},
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Txt"
      ],
      "metadata": {
        "id": "PqHSDsXGRBvZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expecting a copy paste from user"
      ],
      "metadata": {
        "id": "PWseE-RtVssp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Functions"
      ],
      "metadata": {
        "id": "INtQxQkYRMV-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initial Cleaning"
      ],
      "metadata": {
        "id": "X5zFDbhjVLMx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import unicodedata\n",
        "# import re\n",
        "\n",
        "# def clean_jd_text(text):\n",
        "#     # Normalize Unicode (e.g. accents, special characters)\n",
        "#     text = unicodedata.normalize(\"NFKD\", text)\n",
        "\n",
        "#     # Remove zero-width spaces and non-printable characters\n",
        "#     text = re.sub(r'[\\u200B-\\u200D\\uFEFF]', '', text)\n",
        "\n",
        "#     # Remove multiple spaces and fix inconsistent newlines\n",
        "#     text = re.sub(r'[ \\t]+', ' ', text)\n",
        "#     text = re.sub(r'\\n{2,}', '\\n', text)\n",
        "\n",
        "#     # Strip each line\n",
        "#     lines = [line.strip() for line in text.splitlines() if line.strip()]\n",
        "#     return '\\n'.join(lines)\n"
      ],
      "metadata": {
        "id": "DryKRhx2WuJH"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import unicodedata\n",
        "import re\n",
        "\n",
        "def clean_jd_text(text):\n",
        "    # 1. Unicode normalize (NFKD breaks accents into base + diacritic)\n",
        "    text = unicodedata.normalize(\"NFKD\", text)\n",
        "\n",
        "    # 2. Remove zero-width/non-printable characters\n",
        "    text = re.sub(r'[\\u200B-\\u200D\\uFEFF]', '', text)\n",
        "\n",
        "    # 3. Convert curly quotes, en/em dashes, and ellipses to standard equivalents\n",
        "    text = text.replace(\"’\", \"'\").replace(\"‘\", \"'\")\\\n",
        "               .replace(\"“\", '\"').replace(\"”\", '\"')\\\n",
        "               .replace(\"–\", \"-\").replace(\"—\", \"-\")\\\n",
        "               .replace(\"…\", \"...\")\n",
        "\n",
        "    # 4. Remove multiple spaces/tabs, normalize newlines\n",
        "    text = re.sub(r'[ \\t]+', ' ', text)\n",
        "    text = re.sub(r'\\n{2,}', '\\n', text)\n",
        "\n",
        "    # 5. Strip leading/trailing spaces per line, remove empty lines\n",
        "    lines = [line.strip() for line in text.splitlines() if line.strip()]\n",
        "\n",
        "    # 6. Optional: ensure a period ends any standalone sentence-like lines (good before sectioning)\n",
        "    lines = [line if re.search(r'[.!?]$', line) else line + '.' for line in lines]\n",
        "\n",
        "    return '\\n'.join(lines)\n"
      ],
      "metadata": {
        "id": "AaboA9u8eEgl"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Sectioning"
      ],
      "metadata": {
        "id": "4W-UeUCVVLcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def split_into_sections(text):\n",
        "#     sections = {\n",
        "#         \"job info\": \"\",\n",
        "#         \"summary\": \"\",\n",
        "#         \"responsibilities\": \"\",\n",
        "#         \"requirements\": \"\",\n",
        "#         \"education\":\"\",\n",
        "#         \"preferred_qualifications\": \"\"\n",
        "#     }\n",
        "\n",
        "#     patterns = {\n",
        "#         \"summary\": r\"(about the role)\",\n",
        "#         \"responsibilities\": r\"(responsibilities)\",\n",
        "#         \"requirements\": r\"(requirements)\",\n",
        "#         \"education\": r\"(education)\",\n",
        "#         \"preferred_qualifications\": r\"(interests|languages|volunteer|publications|additional information|certifications|nice to have|certifications / nice to have)\"\n",
        "#     }\n",
        "\n",
        "#     lines = text.splitlines()\n",
        "#     current_section = \"job info\"  # Assume resume starts with basic info of the job\n",
        "\n",
        "#     for line in lines:\n",
        "#         clean_line = line.strip().lower()\n",
        "\n",
        "#         # Check if this line is a section header\n",
        "#         for section, pattern in patterns.items():\n",
        "#             if re.search(rf\"^{pattern}[:\\s]*$\", clean_line):\n",
        "#                 current_section = section\n",
        "#                 break\n",
        "#         else:\n",
        "#             sections[current_section] += line + '\\n'\n",
        "\n",
        "#     return sections\n"
      ],
      "metadata": {
        "id": "0EU9-E3IWfau"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import re\n",
        "\n",
        "def split_into_sections(text):\n",
        "    sections = {\n",
        "        \"job info\": \"\",\n",
        "        \"summary\": \"\",\n",
        "        \"responsibilities\": \"\",\n",
        "        \"requirements\": \"\",\n",
        "        \"education\": \"\",\n",
        "        \"preferred_qualifications\": \"\"\n",
        "    }\n",
        "\n",
        "    # Slightly expanded, but still safe and not overfitted\n",
        "    patterns = {\n",
        "        \"summary\": r\"\\b(about the role|summary|job overview)\\b\",\n",
        "        \"responsibilities\": r\"\\b(responsibilities|what you’ll do|your tasks)\\b\",\n",
        "        \"requirements\": r\"\\b(requirements|qualifications|must haves|you have)\\b\",\n",
        "        \"education\": r\"\\b(education|academic background)\\b\",\n",
        "        \"preferred_qualifications\": r\"\\b(preferred qualifications|nice to have|certifications|additional information|interests|languages|volunteer|publications)\\b\"\n",
        "    }\n",
        "\n",
        "    lines = text.splitlines()\n",
        "    current_section = \"job info\"\n",
        "\n",
        "    for line in lines:\n",
        "        clean_line = line.strip().lower()\n",
        "\n",
        "        # Detect section header\n",
        "        matched = False\n",
        "        for section, pattern in patterns.items():\n",
        "            if re.search(pattern, clean_line):\n",
        "                current_section = section\n",
        "                matched = True\n",
        "                break\n",
        "\n",
        "        # If no header match, keep appending to current section\n",
        "        if not matched:\n",
        "            sections[current_section] += line + '\\n'\n",
        "\n",
        "    # Final cleanup\n",
        "    for key in sections:\n",
        "        sections[key] = sections[key].strip()\n",
        "\n",
        "    return sections\n"
      ],
      "metadata": {
        "id": "ByNOjOHdeqHV"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Final cleaning"
      ],
      "metadata": {
        "id": "M8La3iBAVK-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def clean_text(text):\n",
        "#     # text = unidecode.unidecode(text)  # Remove accents\n",
        "#     text = re.sub(r'\\s+', ' ', text)  # Normalize whitespace\n",
        "#     text = re.sub(r'[^a-zA-Z0-9.,;!?()\\[\\] ]+', '', text)  # Remove special characters\n",
        "\n",
        "#     # sentences = re.sub(r'-\\s*', '', text).strip()\n",
        "#     # paragraph= re.sub(r'\\n+','. ',sentences)\n",
        "#     # if not paragraph.endswith('.'):\n",
        "#     #   paragraph+='.'\n",
        "#     # text = text.lower()  # Lowercase\n",
        "#     return text\n",
        "# #cleaning dict values\n",
        "# def clean_dict_values(data_dict):\n",
        "#     return {key: clean_text(value) for key, value in data_dict.items()}\n",
        "# # Assume `sections` is the output from your previous dictionary splitting function\n",
        "\n"
      ],
      "metadata": {
        "id": "1Xf5Vo6VXHeG"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    # Normalize whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text.strip())\n",
        "\n",
        "    # Add period to end of bullet points and sentences if missing\n",
        "    # (Handles '-', '*', or other list formats)\n",
        "    lines = re.split(r'(?<=\\n)|(?<=\\.) ', text)\n",
        "    processed_lines = []\n",
        "\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "\n",
        "        # Add period if it's a bullet or sentence and doesn't end properly\n",
        "        if line:\n",
        "            if line.startswith(('-', '*')):\n",
        "                if not line.endswith('.'):\n",
        "                    line += '.'\n",
        "            elif not re.search(r'[.!?]$', line):\n",
        "                line += '.'\n",
        "            processed_lines.append(line)\n",
        "\n",
        "    # Rejoin the lines as a single string\n",
        "    text = ' '.join(processed_lines)\n",
        "\n",
        "    # Remove unwanted special characters except those useful for sentence context\n",
        "    text = re.sub(r'[^a-zA-Z0-9.,;!?()\\-\\'\\\" ]+', '', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "def clean_dict_values(data_dict):\n",
        "    return {key: clean_text(value) for key, value in data_dict.items()}\n"
      ],
      "metadata": {
        "id": "vSBpuuXocdLI"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Master functions"
      ],
      "metadata": {
        "id": "g5OASDAQRMWB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def txt_processor(text):\n",
        "  jd_raw=text\n",
        "  jd_cleaned=clean_jd_text(jd_raw)\n",
        "  jd_sectioned=split_into_sections(jd_cleaned)\n",
        "  final = clean_dict_values(jd_sectioned)\n",
        "  return final"
      ],
      "metadata": {
        "id": "BcaPxSkyXLqm"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Test"
      ],
      "metadata": {
        "id": "awqEU3roRMWC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "jd_text=\"\"\"Job Title: Junior Web Developer\n",
        "Location: Remote (US-based preferred)\n",
        "Employment Type: Full-Time\n",
        "\n",
        "About the Role\n",
        "We are seeking a passionate Junior Web Developer to join our growing team. This role is ideal for someone who enjoys building responsive, user-friendly web applications and working in a collaborative, agile environment. You will support front-end and back-end development tasks, contribute to code reviews, and help ensure our applications perform efficiently across devices.\n",
        "\n",
        "Responsibilities\n",
        "- Develop and maintain responsive web pages using HTML5, CSS3, and JavaScript\n",
        "- Work with React.js on the front end and Node.js + Express.js on the back end\n",
        "- Design and consume RESTful APIs\n",
        "- Collaborate with UI/UX designers to implement mobile-first designs\n",
        "- Use version control systems like Git and platforms such as GitHub\n",
        "- Write clean, maintainable, and well-documented code\n",
        "- Participate in Agile workflows including daily standups and sprint planning\n",
        "\n",
        "Requirements\n",
        "- Proficiency in HTML5, CSS3, and JavaScript (ES6+)\n",
        "- Familiarity with React.js, Node.js, Express.js\n",
        "- Basic understanding of MongoDB and MySQL\n",
        "- Experience with Git, GitHub, VS Code, and Postman\n",
        "- Comfortable working with APIs and debugging front-end/back-end issues\n",
        "- Strong communication and problem-solving skills\n",
        "- Willingness to learn and grow in a fast-paced development environment\n",
        "\n",
        "Education\n",
        "Bachelor’s degree in Computer Science or related field (or equivalent practical experience)\n",
        "\n",
        "Certifications / Nice to Have\n",
        "- Completion of freeCodeCamp certifications or similar online programs\n",
        "- Internship or freelance experience building real-world web applications\n",
        "- Exposure to tools like Figma, Firebase, or Vercel\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "N1Lan0AGV-23"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Run"
      ],
      "metadata": {
        "id": "iYmpNPTobAmA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "txt_jd_preprocessed=txt_processor(jd_text)\n",
        "txt_jd_preprocessed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQVeLEV2bYAx",
        "outputId": "d516fb29-4aaf-4067-d479-2be9f17a84d1"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'job info': 'Job Title Junior Web Developer. Location Remote (US-based preferred). Employment Type Full-Time.',\n",
              " 'summary': 'We are seeking a passionate Junior Web Developer to join our growing team. This role is ideal for someone who enjoys building responsive, user-friendly web applications and working in a collaborative, agile environment. You will support front-end and back-end development tasks, contribute to code reviews, and help ensure our applications perform efficiently across devices.',\n",
              " 'responsibilities': '- Develop and maintain responsive web pages using HTML5, CSS3, and JavaScript. - Work with React.js on the front end and Node.js  Express.js on the back end. - Design and consume RESTful APIs. - Collaborate with UIUX designers to implement mobile-first designs. - Use version control systems like Git and platforms such as GitHub. - Write clean, maintainable, and well-documented code. - Participate in Agile workflows including daily standups and sprint planning.',\n",
              " 'requirements': '- Proficiency in HTML5, CSS3, and JavaScript (ES6). - Familiarity with React.js, Node.js, Express.js. - Basic understanding of MongoDB and MySQL. - Experience with Git, GitHub, VS Code, and Postman. - Comfortable working with APIs and debugging front-endback-end issues. - Strong communication and problem-solving skills. - Willingness to learn and grow in a fast-paced development environment.',\n",
              " 'education': \"Bachelor's degree in Computer Science or related field (or equivalent practical experience).\",\n",
              " 'preferred_qualifications': '- Internship or freelance experience building real-world web applications. - Exposure to tools like Figma, Firebase, or Vercel.'}"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#jd sectioned"
      ],
      "metadata": {
        "id": "p7n40s2hbNdD"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#jd cleaned"
      ],
      "metadata": {
        "id": "MiEe20O1WEZi"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Semantic Scoring"
      ],
      "metadata": {
        "id": "FYabUF6DRSjB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Function"
      ],
      "metadata": {
        "id": "ZhIinxf3aVdO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# model = SentenceTransformer('all-MiniLM-L6-v2')  # Efficient and accurate\n",
        "model = SentenceTransformer('all-mpnet-base-v2')\n",
        "\n",
        "\n",
        "def compute_semantic_similarity(resume_dict, job_dict):\n",
        "    scores = {}\n",
        "\n",
        "    # Define section pairs to compare\n",
        "    section_pairs = [\n",
        "        ('summary', 'summary'),\n",
        "        ('skills', 'requirements'),\n",
        "        ('experience', 'responsibilities'),\n",
        "        ('certifications', 'preferred_qualifications'),\n",
        "        ('projects', 'responsibilities'),\n",
        "        ('others', 'preferred_qualifications')\n",
        "    ]\n",
        "\n",
        "    for resume_key, job_key in section_pairs:\n",
        "        resume_text = resume_dict.get(resume_key, \"\").strip()\n",
        "        job_text = job_dict.get(job_key, \"\").strip()\n",
        "\n",
        "        if resume_text and job_text:\n",
        "            emb1 = model.encode(resume_text, convert_to_tensor=True)\n",
        "            emb2 = model.encode(job_text, convert_to_tensor=True)\n",
        "            similarity = util.cos_sim(emb1, emb2).item()\n",
        "            scores[f\"{resume_key} vs {job_key}\"] = round(similarity, 4)\n",
        "        else:\n",
        "            scores[f\"{resume_key} vs {job_key}\"] = None  # missing data\n",
        "\n",
        "    return scores\n"
      ],
      "metadata": {
        "id": "yQDjgGFPyjJF"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##get the files"
      ],
      "metadata": {
        "id": "5oiRZEFKaYgC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "job_description=txt_jd_preprocessed.copy()\n",
        "resume=pdf_resume_preprocessed.copy()"
      ],
      "metadata": {
        "id": "z6SRx10XYcwT"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "job_description"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qleEJj9aCDl",
        "outputId": "2823ac9b-a2fa-4862-a306-10bad277adab"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'job info': 'Job Title Junior Web Developer. Location Remote (US-based preferred). Employment Type Full-Time.',\n",
              " 'summary': 'We are seeking a passionate Junior Web Developer to join our growing team. This role is ideal for someone who enjoys building responsive, user-friendly web applications and working in a collaborative, agile environment. You will support front-end and back-end development tasks, contribute to code reviews, and help ensure our applications perform efficiently across devices.',\n",
              " 'responsibilities': '- Develop and maintain responsive web pages using HTML5, CSS3, and JavaScript. - Work with React.js on the front end and Node.js  Express.js on the back end. - Design and consume RESTful APIs. - Collaborate with UIUX designers to implement mobile-first designs. - Use version control systems like Git and platforms such as GitHub. - Write clean, maintainable, and well-documented code. - Participate in Agile workflows including daily standups and sprint planning.',\n",
              " 'requirements': '- Proficiency in HTML5, CSS3, and JavaScript (ES6). - Familiarity with React.js, Node.js, Express.js. - Basic understanding of MongoDB and MySQL. - Experience with Git, GitHub, VS Code, and Postman. - Comfortable working with APIs and debugging front-endback-end issues. - Strong communication and problem-solving skills. - Willingness to learn and grow in a fast-paced development environment.',\n",
              " 'education': \"Bachelor's degree in Computer Science or related field (or equivalent practical experience).\",\n",
              " 'preferred_qualifications': '- Internship or freelance experience building real-world web applications. - Exposure to tools like Figma, Firebase, or Vercel.'}"
            ]
          },
          "metadata": {},
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resume"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1diUTtOaDzG",
        "outputId": "32dac584-14f3-47e1-ea86-7e6061f2c3fb"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'contact': 'Alex Johnson (123) 456-7890, alexjohnson. dev@email. com, github. com/alexjohnson-dev, alexjohnson. dev.',\n",
              " 'summary': 'Junior Web Developer with a strong foundation in front-end and back-end technologies. Skilled in building responsive, user-centric websites and applications. Passionate about learning new technologies and delivering high-quality code.',\n",
              " 'skills': 'This candidate is proficient in Languages: HTML5, CSS3, JavaScript (ES6+), Python Frameworks/Libraries: React. js, Node. js, Express. js, Bootstrap Databases: MongoDB, MySQL Tools: Git, GitHub, Visual Studio Code, Figma, Postman Other: RESTful APIs, Responsive Design, and Agile Methodology.',\n",
              " 'experience': 'Web Development Intern, BrightWave Solutions, Remote, Jan 2024-Apr 2024-Developed and maintained front-end components using React. js and Bootstrap. Built RESTful APIs with Node. js and Express. js. Created mobile-first responsive designs with UX teams. Improved website performance by 20%. Freelance Web Developer, Self-Employed, Remote, May 2023-Dec 2023-Designed and developed custom websites for local businesses. Integrated third-party APIs (Google Maps, Stripe). Provided maintenance and support for client websites.',\n",
              " 'education': 'Bachelor of Science in Computer Science, University of Springfield, Springfield, IL, May 2023 Relevant Coursework: Web Development, Database Systems, Software Engineering, Human-Computer Interaction.',\n",
              " 'certifications': 'I hold certifications including Responsive Web Design Certification (freeCodeCamp, 2022)-JavaScript Algorithms and Data Structures Certification (freeCodeCamp, and 2023).',\n",
              " 'projects': 'TaskMaster Web App-Built a full-stack task management app using React. js, Node. js, and MongoDB. Implemented user authentication and real-time notifications. Personal Portfolio Website-Designed and developed a personal website to showcase projects and blog posts. Focused on accessibility and responsive design.',\n",
              " 'others': 'Other relevant participation includes Open-source contributor, and-Volunteer coding mentor at CodeClub Springfield.'}"
            ]
          },
          "metadata": {},
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##compute"
      ],
      "metadata": {
        "id": "6G0P-eARaawC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "similarity_scores = compute_semantic_similarity(resume, job_description)\n",
        "for k, v in similarity_scores.items():\n",
        "    print(f\"{k}: {v}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8b_h9iMYZSs",
        "outputId": "9a41c6e3-ef4b-4ecd-8828-0d2791865638"
      },
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "summary vs summary: 0.8699\n",
            "skills vs requirements: 0.7566\n",
            "experience vs responsibilities: 0.5541\n",
            "certifications vs preferred_qualifications: 0.5657\n",
            "projects vs responsibilities: 0.6302\n",
            "others vs preferred_qualifications: 0.3649\n"
          ]
        }
      ]
    }
  ]
}